{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DonorsChoose.org (Kaggle Competition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Donors Choose is an institution that supports financially teachers from public schools across United States with their projects. It receives hundreds of applications and has difficult to evaluate all of them. The objective of the work done in the competition is to create a machine learning model to process and evaluate applications automatically. It will save time of the volunteers that has to evaluate application, it is going to leverage faster evaluations and volunteers will be able to better assist teachers that are already with projects on the fly.\n",
    "\n",
    "As most of the data are text data (as teachers write essays to be evaluated), I built a NLP model. For the predction first I tried Naive Bayes but the accuracy was not great, then I tried Random Forest which had a much better accuracy.\n",
    "Due to computer resource restrictions, I ran the model with just a portion of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2728: DtypeWarning: Columns (11,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#importing dataset\n",
    "training_set = pd.read_csv('train.csv')\n",
    "training_set_approved = training_set[training_set.project_is_approved == 1]\n",
    "training_set_disapproved = training_set[training_set.project_is_approved == 0]\n",
    "\n",
    "training_set = pd.concat([training_set_approved[:45000],training_set_disapproved[:45000]], axis=0)\n",
    "\n",
    "\n",
    "test_set = pd.read_csv('test.csv')\n",
    "resources = pd.read_csv('resources.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#grouping and merging datasets (including price of the resource dataset) and rearreging\n",
    "group_resources = resources.groupby(by='id', as_index =False)['price'].agg('sum')\n",
    "training_set_resources = training_set.merge(group_resources, on=['id'], how='left')\n",
    "test_set_resources = test_set.merge(group_resources, on=['id'], how='left')\n",
    "training_set_resources = training_set_resources[['id', 'teacher_id', 'teacher_prefix', 'school_state', 'project_submitted_datetime', 'project_grade_category', 'project_subject_categories', 'project_subject_subcategories', 'project_title', 'project_essay_1', 'project_essay_2', 'project_essay_3', 'project_essay_4', 'project_resource_summary', 'teacher_number_of_previously_posted_projects', 'price', 'project_is_approved']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function for cleaning text that is goint to be analyzed with NLP\n",
    "def cleaning_long_texts(dataset, field):\n",
    "    import re \n",
    "    import nltk\n",
    "    nltk.download('stopwords')\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem.porter import PorterStemmer\n",
    "    field_corpus = []\n",
    "    \n",
    "    print (field)\n",
    "    \n",
    "    for i in range(0, len(dataset)):\n",
    "        field_text = re.sub('[^a-zA-Z]', ' ', dataset[field][i])\n",
    "        field_text = field_text.lower()\n",
    "        field_text = field_text.split()\n",
    "        ps = PorterStemmer()\n",
    "        field_text = [ps.stem(word) for word in field_text if not word in set(stopwords.words('english'))]\n",
    "        field_text = ' '.join(field_text)\n",
    "        field_corpus.append(field_text)\n",
    "        #dataset[field][i] = cv.fit_transform\n",
    "    \n",
    "    return field_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "project_essay_1\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "project_essay_2\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "project_title\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "project_essay_1\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "project_essay_2\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "project_title\n"
     ]
    }
   ],
   "source": [
    "#cleaning texts\n",
    "X_train_project_essay_1 = cleaning_long_texts(training_set_resources, 'project_essay_1')\n",
    "X_train_project_essay_2 = cleaning_long_texts(training_set_resources, 'project_essay_2')\n",
    "X_train_project_title = cleaning_long_texts(training_set_resources, 'project_title')\n",
    "X_test_project_essay_1 = cleaning_long_texts(test_set_resources, 'project_essay_1')\n",
    "X_test_project_essay_2 = cleaning_long_texts(test_set_resources, 'project_essay_2')\n",
    "X_test_project_title = cleaning_long_texts(test_set_resources, 'project_title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#bag of words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features = 3000)    \n",
    "X_train_project_essay_1 = pd.DataFrame(cv.fit_transform(X_train_project_essay_1).toarray())\n",
    "X_train_project_essay_2 = pd.DataFrame(cv.fit_transform(X_train_project_essay_2).toarray())\n",
    "X_train_project_title = pd.DataFrame(cv.fit_transform(X_train_project_title).toarray())\n",
    "X_test_project_essay_1 = pd.DataFrame(cv.fit_transform(X_test_project_essay_1).toarray())\n",
    "X_test_project_essay_2 = pd.DataFrame(cv.fit_transform(X_test_project_essay_2).toarray())\n",
    "X_test_project_title = pd.DataFrame(cv.fit_transform(X_test_project_title).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Adding vectorized text variables to dataframes\n",
    "training_set_resources = pd.concat([training_set_resources,X_train_project_essay_1], axis=1)\n",
    "training_set_resources = pd.concat([training_set_resources,X_train_project_essay_2], axis=1)\n",
    "training_set_resources = pd.concat([training_set_resources,X_train_project_title], axis=1)\n",
    "\n",
    "test_set_resources = pd.concat([test_set_resources,X_test_project_essay_1], axis=1)\n",
    "test_set_resources = pd.concat([test_set_resources,X_test_project_essay_2], axis=1)\n",
    "test_set_resources = pd.concat([test_set_resources,X_test_project_title], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Organizing dataframe\n",
    "training_set_resources['project_is_approved_2'] = training_set_resources['project_is_approved']\n",
    "del training_set_resources['project_is_approved']\n",
    "del training_set_resources['project_essay_1']\n",
    "del training_set_resources['project_essay_2']\n",
    "del training_set_resources['project_essay_3']\n",
    "del training_set_resources['project_essay_4']\n",
    "del training_set_resources['project_resource_summary']\n",
    "del training_set_resources['id']\n",
    "del training_set_resources['teacher_id']\n",
    "del training_set_resources['teacher_prefix']\n",
    "del training_set_resources['project_submitted_datetime']\n",
    "del training_set_resources['project_title']\n",
    "del test_set_resources['project_essay_1']\n",
    "del test_set_resources['project_essay_2']\n",
    "del test_set_resources['project_essay_3']\n",
    "del test_set_resources['project_essay_4']\n",
    "del test_set_resources['project_resource_summary']\n",
    "del test_set_resources['teacher_id']\n",
    "del test_set_resources['teacher_prefix']\n",
    "del test_set_resources['project_submitted_datetime']\n",
    "del test_set_resources['project_title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#independent and dependent variables\n",
    "X = training_set_resources.iloc[:, :-1].values\n",
    "y = training_set_resources.iloc[:, 9006].values\n",
    "Prediction = test_set_resources.iloc[:, 1:9008].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#treating categorical data\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder_X_state = LabelEncoder()\n",
    "X[:,0] = labelencoder_X_state.fit_transform(X[:, 0])\n",
    "\n",
    "labelencoder_X_grade_category = LabelEncoder()\n",
    "X[:,1] = labelencoder_X_grade_category.fit_transform(X[:, 1])\n",
    "\n",
    "labelencoder_X_subject_categories = LabelEncoder()\n",
    "X[:,2] = labelencoder_X_subject_categories.fit_transform(X[:, 2])\n",
    "\n",
    "labelencoder_X_subject_subcategories = LabelEncoder()\n",
    "X[:,3] = labelencoder_X_subject_subcategories.fit_transform(X[:, 3])\n",
    "\n",
    "labelencoder_X_test_state = LabelEncoder()\n",
    "Prediction[:,0] = labelencoder_X_test_state.fit_transform(Prediction[:, 0])\n",
    "\n",
    "labelencoder_X_test_grade_category = LabelEncoder()\n",
    "Prediction[:,1] = labelencoder_X_test_grade_category.fit_transform(Prediction[:, 1])\n",
    "\n",
    "labelencoder_X_test_subject_categories = LabelEncoder()\n",
    "Prediction[:,2] = labelencoder_X_test_subject_categories.fit_transform(Prediction[:, 2])\n",
    "\n",
    "labelencoder_X__test_subject_subcategories = LabelEncoder()\n",
    "Prediction[:,3] = labelencoder_X__test_subject_subcategories.fit_transform(Prediction[:, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#split training set and test set\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "#feature scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "Prediction = sc.fit_transform(Prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fiting random forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier_random = RandomForestClassifier(n_estimators=350, criterion='entropy', random_state=0, n_jobs=-1)\n",
    "classifier_random.fit(X_train, y_train)\n",
    "\n",
    "y_pred_random = classifier_random.predict(X_test)\n",
    "predction_random = classifier_random.predict(Prediction)\n",
    "predction_random_prob = classifier_random.predict_proba(Prediction)\n",
    "predction_random_prob = pd.DataFrame(data=predction_random_prob, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1541 4088]\n",
      " [ 605 8313]]\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm_random = confusion_matrix(y_test, y_pred_random)\n",
    "print (cm_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#k-fold --- naive bayes\n",
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies_random = cross_val_score (estimator = classifier_random, X = X_train, y = y_train, cv=10)\n",
    "accuracies_random.mean()\n",
    "accuracies_random.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#writing result in csv file\n",
    "final_result = pd.concat([test_set_resources['id'],predction_random_prob[1]], axis=1)\n",
    "final_result['project_is_approved'] = final_result[1]\n",
    "del final_result[1]\n",
    "fname = \"final_result.csv\"\n",
    "final_result.to_csv(fname, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
